# Megakernels!

## Throughput Llama

We may at some point write proper docs on how to use these Megakernels, but for the time-being we're open-sourcing the code for research reference, as opposed to a product that we intend to support.

## Low-Latency Llama Demo

Please use the other branch!
